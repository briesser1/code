
# AI in Healthcare: Presentation Outline

## Section 1: Traditional Machine Learning vs. Generative AI

### 1.1 Introduction to AI Paradigms
- Definition of Artificial Intelligence as the umbrella term
- Historical evolution from rule-based systems to modern AI

### 1.2 Traditional Machine Learning
- **Definition**: Systems that learn patterns from data without explicit programming
- **Key Algorithms**:
  - XGBoost (Gradient Boosted Decision Trees)
  - Random Forests
  - Support Vector Machines
  - Traditional Neural Networks (shallow networks)
- **Characteristics**:
  - Requires structured, curated datasets
  - Feature engineering is critical
  - Produces classifications, predictions, point estimates
  - Interpretable outputs (especially tree-based models)

### 1.3 Deep Learning & Neural Networks
- Multi-layer neural networks
- Automatic feature learning from raw data
- Excels at unstructured data (images, audio, text)
- Examples: CNNs for medical imaging, RNNs for time-series

### 1.4 Generative AI & Large Language Models (LLMs)
- **Definition**: AI that creates new content (text, images, code)
- **Key Characteristics**:
  - Trained on massive datasets (internet-scale)
  - Produces natural language, images, music
  - Transformer architecture foundation
  - In-context learning and emergent capabilities
- **Examples**: GPT-4, Claude, Llama, Med-PaLM

### 1.5 Comparison Matrix
| Aspect | Traditional ML | Deep Learning | Generative AI/LLMs |
|--------|---------------|---------------|-------------------|
| Data Size | Small-medium | Large | Massive |
| Output | Predictions | Classifications | Generated content |
| Interpretability | High | Low | Low |
| Resource Needs | Low-moderate | High | Very high |
| Best For | Structured data | Images/audio | Language tasks |

---

## Section 2: Cutting-Edge LLM Methods and Infrastructure

### 2.1 LLM Orchestration Frameworks

#### LangChain
- Purpose: Rapid prototyping and RAG applications
- Composable chains using the pipe (|) operator
- 80K+ GitHub stars, extensive ecosystem
- Best for: Quick prototypes, RAG pipelines

#### LangGraph
- State management for complex agent workflows
- Graph-based architecture (nodes and edges)
- General availability May 2025
- Used by: LinkedIn, Uber, Replit
- Best for: Production multi-agent systems

#### Claude Code
- Terminal-based agentic coding assistant
- Full codebase understanding
- Git operations and DevOps workflows
- Checkpoint system for code recovery
- Subagents for parallel development

#### Model Context Protocol (MCP)
- Open standard introduced by Anthropic (November 2024)
- "USB-C for AI" - universal interface for LLM-tool connections
- Industry adoption: OpenAI (March 2025), Google DeepMind (April 2025), Microsoft (May 2025)
- Donated to Linux Foundation's Agentic AI Foundation (December 2025)
- 97M+ monthly SDK downloads, 5,800+ MCP servers available

### 2.2 Local LLMs: Running Models On-Premises

#### Why Local LLMs Matter for Healthcare
- **Zero data exposure**: Patient data never leaves your network
- **Compliance by design**: HIPAA, GDPR, SOC 2 met automatically
- **Air-gapped deployment**: Can run in isolated environments
- **No per-query costs**: Fixed hardware investment

#### Local Deployment Tools
| Tool | Best For | Key Features |
|------|----------|--------------|
| Ollama | Development, prototyping | Simple CLI, MIT licensed, no telemetry |
| vLLM | Production multi-user | 3x better throughput, enterprise-grade |
| llama.cpp | Maximum control, edge | C++ implementation, runs on CPU |
| LM Studio | Desktop users | GUI interface, easy management |

#### Popular Open-Source Models
- **Llama 3.1/3.2** (Meta): 8B-405B parameters, tool calling support
- **Mistral/Mixtral** (Mistral AI): Excellent efficiency, function calling
- **Qwen 2.5** (Alibaba): Strong multilingual capabilities
- **Phi-3** (Microsoft): Small but capable (3.8B), modest hardware

#### When to Choose Local vs. Cloud
| Scenario | Recommendation | Reason |
|----------|----------------|--------|
| Processing PHI/patient data | **Local** | Data never leaves network |
| Prototype or MVP | Cloud API | Lower barrier to start |
| High-volume, predictable load | **Local** | Cost per query drops |
| Need frontier capabilities | Cloud API | Largest models available |

#### Hybrid Approach
- Local LLMs for sensitive tasks (patient records, clinical notes)
- Cloud APIs for general tasks (literature search, admin content)

#### Other Frameworks
- LlamaIndex: Best for document-heavy RAG
- CrewAI: Role-based multi-agent orchestration
- OpenAI Agents SDK: Lightweight Python framework

### 2.3 Encoder Models (BERT) vs. Decoder Models (Local LLMs)

#### Architectural Differences
| Aspect | BERT (Encoder-Only) | Local LLMs (Decoder-Only) |
|--------|---------------------|--------------------------|
| Architecture | Transformer encoder | Transformer decoder |
| Attention | Bidirectional (sees full context) | Causal (left-to-right only) |
| Training Objective | Masked Language Modeling (MLM) | Next-token prediction |
| Primary Output | Embeddings, classifications | Generated text |
| Parameter Count | 110M–395M | 7B–405B |

#### When to Use BERT-Style Encoders
- **Classification tasks**: Sentiment analysis, intent detection, medical coding
- **Named Entity Recognition**: Extracting medications, diagnoses, procedures from clinical notes
- **Semantic search**: Creating embeddings for RAG retrieval
- **Structured prediction**: Tasks with fixed output categories
- **Resource-constrained environments**: Can run on modest hardware

#### When to Use Local LLMs (Llama, Mistral)
- **Text generation**: Clinical note drafting, patient communication
- **Conversational AI**: Chatbots, Q&A systems
- **Zero-shot/few-shot tasks**: No fine-tuning required
- **Complex reasoning**: Multi-step inference
- **Open-ended responses**: Summarization, explanation generation

#### Healthcare-Specific BERT Models
| Model | Training Data | Best For |
|-------|---------------|----------|
| BioBERT | PubMed abstracts | Biomedical literature mining |
| ClinicalBERT | MIMIC-III clinical notes | EHR analysis |
| PubMedBERT | PubMed (from scratch) | Biomedical NLP tasks |
| Clinical ModernBERT | MIMIC-IV + PubMed + ontologies | Long clinical documents (8K context) |

#### Resource Requirements Comparison
| Factor | BERT Fine-tuning | LLM Inference | LLM Fine-tuning |
|--------|------------------|---------------|-----------------|
| GPU Memory | 8–16 GB | 14–80 GB | 40–160+ GB |
| Training Time | Hours | N/A (prompting) | Days |
| Hardware | Consumer GPU | Gaming/Pro GPU | Enterprise GPU |
| Cost | Low | Medium | High |

#### Key Decision Framework
- **Use BERT when**: You need classification, embeddings, or NER with limited compute
- **Use Local LLMs when**: You need text generation, conversation, or zero-shot capabilities
- **Hybrid approach**: BERT for embeddings/retrieval + LLM for generation (RAG architecture)

#### ModernBERT: The 2024 Upgrade
- 8,192 token context (vs. 512 for original BERT)
- Up to 400% faster than original BERT
- 80% less memory than DeBERTaV3
- First encoder with code in training data
- Slot-in replacement for existing BERT deployments

### 2.4 Understanding the Integration Stack

#### Direct API Calls
- Simplest approach: code directly calls LLM API
- Full control, no abstraction overhead
- Each tool requires custom integration code
- Best for: Simple applications, prototypes

#### Orchestration Frameworks (LangChain/LangGraph)
- Handles reasoning loops (Thought → Action → Observation)
- State management and memory
- Tools must be pre-defined in code
- Best for: Complex reasoning workflows

#### Model Context Protocol (MCP)
- Protocol-based, enables runtime tool discovery
- Solves "M×N problem": 10 apps + 100 tools = 110 integrations (not 1,000)
- Client-server architecture (AI app = client, tools = servers)
- Best for: Enterprise environments with shared tooling

#### How They Work Together
- Direct API calls for simple interactions
- LangChain/LangGraph for orchestration
- MCP as the integration layer

### 2.5 Retrieval-Augmented Generation (RAG)
- Combines retrieval systems with generative models
- Grounds LLM responses in factual data
- Reduces hallucinations
- Workflow: Query → Retrieve → Augment → Generate

### 2.6 Embeddings Explained
- **Definition**: Numerical vector representations of data
- **How They Work**:
  - Neural networks convert text/images to vectors
  - Similar items cluster together in vector space
  - Cosine similarity measures closeness
- **Applications**:
  - Semantic search (meaning, not keywords)
  - RAG retrieval
  - Classification and clustering
- **Evolution**: word2vec (2013) → BERT → Modern contextual embeddings

### 2.7 Vector Databases
- **Purpose**: Store and search high-dimensional vectors
- **Operations**: Nearest neighbor search, similarity matching
- **Strengths**:
  - Fast similarity search at scale
  - Handles unstructured data well
  - Integrates with ML pipelines
- **Limitations**:
  - "Black box" - hard to trace errors
  - May retrieve incorrect chunks
- **Examples**: Pinecone, Weaviate, Milvus, ChromaDB

### 2.8 Graph Databases
- **Purpose**: Store entities and relationships as nodes/edges
- **Operations**: Graph traversal, pattern matching
- **Strengths**:
  - Transparent and explainable
  - Captures complex relationships
  - Easy error tracing
- **Limitations**:
  - Complex data modeling required
  - Struggles with large-scale processing
- **Examples**: Neo4j, FalkorDB, Amazon Neptune

### 2.9 Hybrid Architectures (GraphRAG)
- Combines vector similarity with knowledge graphs
- Best of both: semantic search + relationship awareness
- Emerging best practice for enterprise applications

---

## Section 3: Healthcare AI Use Cases and Deployments

### 3.1 Python Libraries for Healthcare AI

#### PyHealth
- Deep learning toolkit for healthcare
- Supports MIMIC-III/IV, eICU, OMOP-CDM
- Tasks: Drug recommendation, mortality prediction, readmission
- 10 lines of code for full pipeline

#### MONAI (Medical Open Network for AI)
- PyTorch-based medical imaging framework
- NVIDIA-backed, clinical-grade deployment ready
- Standardized tools for research to production

#### Other Key Libraries
- TensorFlow/PyTorch: Core deep learning
- Pandas/NumPy: Data manipulation
- Biopython: Bioinformatics and genomics
- Scikit-learn: Traditional ML algorithms

### 3.2 Large-Scale Healthcare AI Deployments

#### Microsoft/Nuance DAX Copilot + Epic
- **Deployment**: 150+ health systems, 77% of US hospitals
- **Function**: Ambient AI clinical documentation
- **Results**: 50% reduction in documentation time, 70% reduction in burnout
- **Partners**: Stanford Medicine, Providence, UNC Health

#### Major Health System Adopters
- **Cleveland Clinic**: AI for diagnostics and operations
- **Mayo Clinic**: AI-powered clinical decision support
- **Kaiser Permanente**: Predictive analytics for population health
- **Mass General Brigham**: Microsoft TRAIN initiative participant

#### Startup Disruption
- **Abridge**: Clinical documentation AI
- **Ambience Healthcare**: Ambient AI platform
- 70% of new ambient AI market captured by startups

### 3.3 Market Statistics (2024-2025)
- Healthcare AI spending: $1.4 billion in 2025 (3x increase)
- 22% of healthcare organizations using domain-specific AI
- 85% of generative AI spend goes to startups
- Epic holds 37-39% of acute care hospital market

### 3.4 Case Study: AI Healthcare Failure

#### IBM Watson for Oncology - The $4 Billion Lesson
- **Investment**: $5+ billion in acquisitions and development
- **Sale Price**: ~$1 billion (2022)
- **Net Loss**: ~$4 billion

**What Went Wrong**:
1. **Synthetic Training Data**: Used hypothetical cases, not real patient data
2. **Poor Performance**: 12-96% concordance with oncologists (highly variable)
3. **Dangerous Recommendations**: Memorial Sloan Kettering called some suggestions "useless and dangerous"
4. **Inability to Adapt**: Could not incorporate new treatments or local practices
5. **Cost**: MSK spent $62 million before acknowledging failure

**Lessons Learned**:
- Real-world data is essential
- External validation across institutions is critical
- AI must integrate into existing clinical workflows
- Continuous learning and updating required
- Start with narrow, well-defined problems

### 3.5 Other Notable Failures
- **Epic Sepsis Model**: 67% miss rate, 18% false alert rate
- **Google Diabetic Retinopathy (Thailand)**: 21% image rejection due to poor conditions
- **General Finding**: 85% of AI models fail due to data quality (Gartner)

---

## Conclusion
- Traditional ML vs. Generative AI: Different tools for different problems
- Encoder (BERT) vs. Decoder (LLM) models serve distinct purposes
- LLM orchestration frameworks enable complex AI applications
- Healthcare AI showing real promise but requires careful implementation
- Failures teach us about data quality, validation, and workflow integration

---

## Citations
[Detailed citation list to be included in final document]
